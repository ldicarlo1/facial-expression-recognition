{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "systematic-tongue",
   "metadata": {},
   "source": [
    "# GFE Classification: Training & Testing Using Neural Networkss\n",
    "\n",
    "#####  This notebook aims to use a Convolutional Nueral Network (CNN) to classify facial captured footage as a specified emotion. Data exploration, training, and modelling will all be discussed below and mitigations will be provided. \n",
    "###### (i) Train/test SVM on GFE data on a single emotion and evaluate performance measures\n",
    "###### (ii) Repeat test on a different facial expression\n",
    "###### (iii) Invert the roles of the Users\n",
    "###### (iiii) Use Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import plotly.express as px\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold, cross_val_score\n",
    "from keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-prisoner",
   "metadata": {},
   "source": [
    "## Train on User A and Test on User B \"Negative\" \n",
    "\n",
    "##### Load dataset and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select emotion\n",
    "emotion = \"negative\" \n",
    "\n",
    "# read in data file\n",
    "df_neg = pd.read_csv(f\"grammatical_facial_expression/a_{emotion}_datapoints.txt\",delimiter = \" \",)\n",
    "df_neg_target = pd.read_csv(f\"grammatical_facial_expression/a_{emotion}_targets.txt\",delimiter = \" \",header=None)\n",
    "\n",
    "# read in User B datafile\n",
    "df_neg_userb = pd.read_csv(f\"grammatical_facial_expression/b_{emotion}_datapoints.txt\",delimiter = \" \",)\n",
    "df_neg_target_userb = pd.read_csv(f\"grammatical_facial_expression/b_{emotion}_targets.txt\",delimiter = \" \",header=None)\n",
    "\n",
    "# combine both dataframes using the target dataset\n",
    "df_neg['target'] = df_neg_target\n",
    "df_neg_userb['target'] = df_neg_target_userb\n",
    "\n",
    "# split train/test and validation\n",
    "X_neg = df_neg.iloc[:,1:-1]\n",
    "y_neg = df_neg.iloc[:,-1]\n",
    "X_neg_userb = df_neg_userb.iloc[:,1:-1]\n",
    "y_neg_userb = df_neg_userb.iloc[:,-1]\n",
    "\n",
    "# scale data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_neg_scaled = scaler.fit_transform(X_neg)\n",
    "X_neg_scaled_userb = scaler.fit_transform(X_neg_userb)\n",
    "\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data for CNN\n",
    "# Add additional dimension as holder dimension for CNN\n",
    "X_neg_scaled_transform = np.expand_dims(X_neg_scaled,axis=2)\n",
    "X_neg_scaled_transform_userb = np.expand_dims(X_neg_scaled_userb,axis=2)\n",
    "\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# tHot encode the output to produce a two class probability response to represent the binary output\n",
    "y_neg_onehot = encoder.fit_transform(df_neg_target)\n",
    "y_neg_onehot_userb = encoder.fit_transform(df_neg_target_userb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-courage",
   "metadata": {},
   "source": [
    "## Generate NN\n",
    "Create NN below and create function to produce performance plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(activation, dropout_rate=0.2):\n",
    "    # define model instance\n",
    "    model = Sequential()\n",
    "\n",
    "    # add 1D conv layer 300 in length \n",
    "    model.add(Dense(300,activation = activation, input_shape=(300,1)))\n",
    "\n",
    "    # add dropout layer for regularization to avoid over-fitting\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # add 2nd layer 100 length\n",
    "    model.add(Dense(100,activation = activation, input_shape = (100,1)))\n",
    "\n",
    "    # add dropout layer \n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # 3rd layer 10 in length\n",
    "    model.add(Dense(10, activation=activation))\n",
    "\n",
    "\n",
    "    # Add dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = activation))\n",
    "\n",
    "    # use softmax to create two class output based on probability\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_loss(model, emotion, user):\n",
    "    stats = pd.DataFrame(model.history)\n",
    "    stats.reset_index(level=0, inplace=True)\n",
    "    stats = stats.melt(id_vars =['index'])\n",
    "    stats.columns= ['Epoch','metric', 'Value']\n",
    "\n",
    "    fig = px.line(stats,x='Epoch', y=\"Value\", color='metric',title=f\"NN Model Statistics for {emotion}: {user}\")\n",
    "    fig.update_yaxes(range=[0, 1])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-connection",
   "metadata": {},
   "source": [
    "##### Use GridSearchCV to hyperparameter tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model as Keras object\n",
    "model = KerasClassifier(build_fn=NN_model, verbose=True, epochs=8, batch_size = 100)\n",
    "\n",
    "# define parameter dictionaries\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# create param grid\n",
    "param_grid = dict(activation=activation, dropout_rate = dropout)\n",
    "\n",
    "# grid search \n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=5)\n",
    "\n",
    "# Fit data\n",
    "grid_result = grid.fit(X_neg_scaled_transform, y_neg_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return best results\n",
    "print(f\"Best hyperparameter values: {grid_result.best_params_}\")\n",
    "print(f\"Cross fold validation best score {grid_result.cv_results_['mean_test_score'][grid_result.best_index_]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-research",
   "metadata": {},
   "source": [
    "Best hyperparameter values: {'activation': 'relu', 'dropout_rate': 0.3}\n",
    "Cross fold validation best score 0.8851944327354431\n",
    "\n",
    "5-Cross fold validation identified 'relu' as the best activation function and a dropout_rate of 0.3 to be the best model fit.\n",
    "\n",
    "### Train/Test on \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instance with tuned hyperparameters\n",
    "model_neg_usera = NN_model(activation='relu',dropout_rate=0.3)\n",
    "\n",
    "# fit model data\n",
    "model_train = model_neg_usera.fit(X_neg_scaled_transform,y_neg_onehot,validation_split=0.2,batch_size = 100, epochs = 20, verbose=False)\n",
    "\n",
    "# produce plot of performance\n",
    "plot_loss(model_train, 'Negative','User A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-latest",
   "metadata": {},
   "source": [
    "The performance statistics above indicate that after 5 epochs the validation loss begins. \n",
    "\n",
    "###### Test Final Model for Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine final model with optimal epochs\n",
    "model_neg_usera = NN_model(activation='relu',dropout_rate=0.3)\n",
    "final_model = model_neg_usera.fit(X_neg_scaled_transform,y_neg_onehot,batch_size=100,epochs=6, verbose = False)\n",
    "\n",
    "# save final model weights\n",
    "#model_neg_usera.save_weights('models/nn_neg_usera.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model_neg_usera.load_weights(\"models/nn_neg_usera.h5\")\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_neg_usera.predict(X_neg_scaled_transform_userb)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_neg_userb, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_neg_userb, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_neg_userb, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_neg_userb, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-start",
   "metadata": {},
   "source": [
    "Testing this model with User B data provides an accuracy of 60.8%, precision of 55.4%, and recall of 57.7%. Compared to the SVM results, this indicates little difference in performance between the two models.\n",
    "\n",
    "## Train on User A and Test on User B \"Emphasis\"\n",
    "\n",
    "##### Load dataset and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select emotion\n",
    "emotion = \"emphasis\" \n",
    "\n",
    "# read in data file\n",
    "df_emp = pd.read_csv(f\"grammatical_facial_expression/a_{emotion}_datapoints.txt\",delimiter = \" \",)\n",
    "df_emp_target = pd.read_csv(f\"grammatical_facial_expression/a_{emotion}_targets.txt\",delimiter = \" \",header=None)\n",
    "\n",
    "# read in User B datafile\n",
    "df_emp_userb = pd.read_csv(f\"grammatical_facial_expression/b_{emotion}_datapoints.txt\",delimiter = \" \",)\n",
    "df_emp_target_userb = pd.read_csv(f\"grammatical_facial_expression/b_{emotion}_targets.txt\",delimiter = \" \",header=None)\n",
    "\n",
    "# combine both dataframes using the target dataset\n",
    "df_emp['target'] = df_emp_target\n",
    "df_emp_userb['target'] = df_emp_target_userb\n",
    "\n",
    "# split train/test and validation\n",
    "X_emp = df_emp.iloc[:,1:-1]\n",
    "y_emp = df_emp.iloc[:,-1]\n",
    "X_emp_userb = df_emp_userb.iloc[:,1:-1]\n",
    "y_emp_userb = df_emp_userb.iloc[:,-1]\n",
    "\n",
    "# scale data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_emp_scaled = scaler.fit_transform(X_emp)\n",
    "X_emp_scaled_userb = scaler.fit_transform(X_emp_userb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data for CNN\n",
    "# Add additional dimension as holder dimension for CNN\n",
    "X_emp_scaled_transform = np.expand_dims(X_emp_scaled,axis=2)\n",
    "X_emp_scaled_transform_userb = np.expand_dims(X_emp_scaled_userb,axis=2)\n",
    "\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# tHot encode the output to produce a two class probability response to represent the binary output\n",
    "y_emp_onehot = encoder.fit_transform(df_emp_target)\n",
    "y_emp_onehot_userb = encoder.fit_transform(df_emp_target_userb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-miami",
   "metadata": {},
   "source": [
    "#### Train NN model\n",
    "\n",
    "The previous model architecture will be applied to \"Emphasis\" to see if the model is flexible enough to perform on multiple emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-define model with previously tuned hyperparameters\n",
    "model_emp_usera = NN_model(activation='relu',dropout_rate=0.3)\n",
    "model_training = model_emp_usera.fit(X_emp_scaled_transform, y_emp_onehot,validation_split=0.2,batch_size=100,epochs=30,verbose=False)\n",
    "\n",
    "# plot loss\n",
    "plot_loss(model_training,'Emphasis', 'User A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-undergraduate",
   "metadata": {},
   "source": [
    "The performance statistics above indicate that after 10 epochs the validation loss bottoms out, indicating that the model may begin overfitting after.\n",
    "\n",
    "###### Test Final Model for Emphasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine final model\n",
    "model_emp_usera = NN_model(activation='relu',dropout_rate=0.3)\n",
    "final_model = model_emp_usera.fit(X_emp_scaled_transform,y_emp_onehot,batch_size=100,epochs=10,validation_split=.2, verbose=False)\n",
    "\n",
    "# save final model weights\n",
    "#model_emp_usera.save_weights('models/nn_emp_usera.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model_emp_usera.load_weights(\"models/nn_emp_usera.h5\")\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_emp_usera.predict(X_emp_scaled_transform_userb)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_emp_userb, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_emp_userb, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_emp_userb, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_emp_userb, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-metropolitan",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.8727678571428571   \n",
    "Model Precision: 0.8237410071942446  \n",
    "Model Recall: 0.8625235404896422    \n",
    "Model AUC Score: 0.8709911675387941   \n",
    "\n",
    "The results indicate a well-performing model with 87.3% accuracy, 82.3% precision, and 86.3% recall. The AUC of the ROC is 87.1%, which is good. Overall the model is well fitting, and this suggests that the model is flexible enough to perform on different emotions.\n",
    "\n",
    "## Train on User B and Test on User A\n",
    "#### Negative Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model instance from above\n",
    "# fit model data\n",
    "model_neg_userb = NN_model(activation='relu',dropout_rate=0.3)\n",
    "model_train = model_neg_userb.fit(X_neg_scaled_transform_userb,y_neg_onehot_userb,validation_split=0.2,batch_size = 100, epochs = 15, verbose=False)\n",
    "\n",
    "# produce plot of performance\n",
    "plot_loss(model_train, 'Negative', \"User B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-darkness",
   "metadata": {},
   "source": [
    "Model training on User B is very poor, as validation loss increases throughout each epoch. Use standard epoch = 5.\n",
    "\n",
    "###### Determine Final Model for Negative User B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine final model\n",
    "model_neg_userb.fit(X_neg_scaled_transform_userb,y_neg_onehot_userb,batch_size=100,epochs=5,validation_split=.2, verbose=False)\n",
    "\n",
    "# save final model weights\n",
    "#model_neg_userb.save_weights('models/nn_neg_userb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "final_model = model_neg_userb.load_weights(\"models/nn_neg_userb.h5\")\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_neg_userb.predict(X_neg_scaled_transform)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_neg, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_neg, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_neg, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_neg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-arbitration",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.5569395017793595  \n",
    "Model Precision: 0.519280205655527  \n",
    "Model Recall: 0.7651515151515151   \n",
    "Model AUC Score: 0.5688173683140125  \n",
    "\n",
    "Model accuracy when training on User B and testing on User A for \"Negative\" results in a poor model with 55.6% accuracy, 51.9.3% precision, and 76.5% recall.\n",
    "\n",
    "#### Emphasis Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model instance from above\n",
    "# fit model data\n",
    "model_emp_userb = NN_model(activation='relu',dropout_rate=0.3)\n",
    "model_train = model_emp_userb.fit(X_emp_scaled_transform_userb,y_emp_onehot_userb,validation_split=0.2,batch_size = 100, epochs = 20, verbose=False)\n",
    "\n",
    "# produce plot of performance\n",
    "plot_loss(model_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-migration",
   "metadata": {},
   "source": [
    "Model performance indicates that after 2 epochs the validation loss and accuracy plateaus. Therefore use epoch 2 or 3.\n",
    "\n",
    "###### Determine Final Model for Emphasis User B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model\n",
    "model = NN_model(activation='relu',dropout_rate=0.3)\n",
    "final_model = model_emp_userb.fit(X_emp_scaled_transform_userb,y_emp_onehot_userb,batch_size=100,epochs=2,validation_split=.2, verbose=False)\n",
    "\n",
    "# save final model weights\n",
    "#model_emp_userb.save_weights('models/nn_emp_userb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model_emp_userb.load_weights(\"models/nn_emp_userb.h5\")\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_emp_userb.predict(X_emp_scaled_transform)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_emp, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_emp, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_emp, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_emp, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-enemy",
   "metadata": {},
   "source": [
    "Model accuracy for \"Emphasis\" when testing on User A indicates an accuracy of 71.6%, precision of 40.6%, recall of 44.5%, and AUC of 62.2%. Overall this is a poor fitting model.\n",
    "\n",
    "The neural network that was built above robust enough to perform well on both \"Negative\" and \"Emphasis\" emotions when training on User A and testing on User B. However when training on User B and testing on User A, the model failed to perform well on both emotions.\n",
    "\n",
    "# Different Feature Representation: Multiple Input Neural Network\n",
    "\n",
    "Using the paper provided in the assigment, the data will be fed to the model by facial feature (eye, mouth, nose, etc). There are 10 facial features identified in the paper of the dataset, and each input will have a two dimensional shape for each coordinate axis (x,y,z) and the number of corresponding datapoints per facial feature.\n",
    "\n",
    "This method will be tested for the emotions \"Negative\" and \"Emphasis\" to see if the model is flexible enough to perform on both emotions, and also will be trained and tested on both User A and User B to see if this model can perform well when trained on both users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data by x,y, and z coordinates\n",
    "xs = X_neg_scaled[:,0::3]\n",
    "ys = X_neg_scaled[:,1::3]\n",
    "zs = X_neg_scaled[:,2::3]\n",
    "\n",
    "# combine data into 3-dimensional array shape (# of rows, # of dimensions, number of datapoints per dimension)\n",
    "X_neg_scaled_coord = np.swapaxes(np.array((xs,ys,zs)),0,1)\n",
    "\n",
    "# Perform on user B for testing\n",
    "xs_userb = X_neg_scaled_userb[:,0::3]\n",
    "ys_userb = X_neg_scaled_userb[:,1::3]\n",
    "zs_userb = X_neg_scaled_userb[:,2::3]\n",
    "\n",
    "X_neg_scaled_coord_userb = np.swapaxes(np.array((xs_userb,ys_userb,zs_userb)),0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-brighton",
   "metadata": {},
   "source": [
    "##### Split data into 10 facial features based on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For User A Negative\n",
    "left_eye =  X_neg_scaled_coord[:,:,0:8]\n",
    "right_eye = X_neg_scaled_coord[:,:,8:16]\n",
    "left_eyebrow =  X_neg_scaled_coord[:,:,16:26]\n",
    "right_eyebrow =  X_neg_scaled_coord[:,:,26:36]\n",
    "nose = X_neg_scaled_coord[:,:,36:48]\n",
    "mouth = X_neg_scaled_coord[:,:,48:68]\n",
    "face_contour = X_neg_scaled_coord[:,:,68:87]\n",
    "iris = X_neg_scaled_coord[:,:,87:90]\n",
    "left_line = X_neg_scaled_coord[:,:,90:95]\n",
    "right_line = X_neg_scaled_coord[:,:,95:100]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For User B Negative\n",
    "left_eye_userb =  X_neg_scaled_coord_userb[:,:,0:8]\n",
    "right_eye_userb = X_neg_scaled_coord_userb[:,:,8:16]\n",
    "left_eyebrow_userb =  X_neg_scaled_coord_userb[:,:,16:26]\n",
    "right_eyebrow_userb =  X_neg_scaled_coord_userb[:,:,26:36]\n",
    "nose_userb = X_neg_scaled_coord_userb[:,:,36:48]\n",
    "mouth_userb = X_neg_scaled_coord_userb[:,:,48:68]\n",
    "face_contour_userb = X_neg_scaled_coord_userb[:,:,68:87]\n",
    "iris_userb = X_neg_scaled_coord_userb[:,:,87:90]\n",
    "left_line_userb = X_neg_scaled_coord_userb[:,:,90:95]\n",
    "right_line_userb = X_neg_scaled_coord_userb[:,:,95:100]\n",
    "\n",
    "# define 10 facial features \n",
    "train = [left_eye,right_eye,left_eyebrow,right_eyebrow,nose,mouth,face_contour,iris,left_line,right_line]\n",
    "train_userb = [left_eye_userb,right_eye_userb,left_eyebrow_userb,right_eyebrow_userb,nose_userb,mouth_userb,face_contour_userb,iris_userb, left_line_userb, right_line_userb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-execution",
   "metadata": {},
   "source": [
    "###### Multiple Input Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_NN_model(activation = \"relu\", dropout_rate=0.3):\n",
    "    # define the activation function as tanh (best performing)\n",
    "    # define 10 inputs per facial expression, and the given shapes\n",
    "    input_left_eye = Input(shape=(3,8))\n",
    "    input_right_eye = Input(shape=(3,8))\n",
    "    input_left_eyebrow = Input(shape=(3,10))\n",
    "    input_right_eyebrow = Input(shape=(3,10))\n",
    "    input_nose = Input(shape=(3,12))\n",
    "    input_mouth = Input(shape=(3,20))\n",
    "    input_face_contour = Input(shape=(3,19))\n",
    "    input_iris = Input(shape=(3,3))\n",
    "    input_left_line = Input(shape=(3,5))\n",
    "    input_right_line = Input(shape=(3,5))\n",
    "\n",
    "    # define input layer and produce outputs of 1 for each in order to be concatenated into a hidden layer of length 10\n",
    "    left_eye = Dense(8,activation=activation)(input_left_eye)\n",
    "    left_eye = Dense(1,activation=activation)(left_eye)\n",
    "    left_eye = Model(inputs=input_left_eye, outputs=left_eye)\n",
    "\n",
    "    right_eye = Dense(8,activation=activation)(input_right_eye)\n",
    "    right_eye = Dense(1,activation=activation)(right_eye)\n",
    "    right_eye = Model(inputs=input_right_eye, outputs=right_eye)\n",
    "\n",
    "    left_eyebrow = Dense(10,activation=activation)(input_left_eyebrow)\n",
    "    left_eyebrow = Dense(1,activation=activation)(left_eyebrow)\n",
    "    left_eyebrow = Model(inputs=input_left_eyebrow, outputs=left_eyebrow)\n",
    "\n",
    "    right_eyebrow = Dense(10,activation=activation)(input_right_eyebrow)\n",
    "    right_eyebrow = Dense(1,activation=activation)(right_eyebrow)\n",
    "    right_eyebrow = Model(inputs=input_right_eyebrow, outputs=right_eyebrow)\n",
    "\n",
    "    nose = Dense(12,activation=activation)(input_nose)\n",
    "    nose = Dense(1,activation=activation)(nose)\n",
    "    nose = Model(inputs=input_nose, outputs=nose)\n",
    "\n",
    "    mouth = Dense(20,activation=activation)(input_mouth)\n",
    "    mouth = Dense(1,activation=activation)(mouth)\n",
    "    mouth = Model(inputs=input_mouth, outputs=mouth)\n",
    "\n",
    "    face_contour = Dense(19,activation=activation)(input_face_contour)\n",
    "    face_contour = Dense(1,activation=activation)(face_contour)\n",
    "    face_contour = Model(inputs=input_face_contour, outputs=face_contour)\n",
    "\n",
    "    iris = Dense(3,activation=activation)(input_iris)\n",
    "    iris = Dense(1,activation=activation)(iris)\n",
    "    iris = Model(inputs=input_iris, outputs=iris)\n",
    "\n",
    "    left_line = Dense(3,activation=activation)(input_left_line)\n",
    "    left_line = Dense(1,activation=activation)(left_line)\n",
    "    left_line = Model(inputs=input_left_line, outputs=left_line)\n",
    "\n",
    "    right_line = Dense(3,activation=activation)(input_right_line)\n",
    "    right_line = Dense(1,activation=activation)(right_line)\n",
    "    right_line = Model(inputs=input_right_line, outputs=right_line)\n",
    "\n",
    "    # concatenate outputs and define layer 1\n",
    "    layer1 = concatenate([left_eye.output, right_eye.output, left_eyebrow.output,right_eyebrow.output, \n",
    "                          nose.output, mouth.output,face_contour.output,iris.output,left_line.output,right_line.output])\n",
    "    \n",
    "    # add dropout to avoid overfit\n",
    "    layer1 = BatchNormalization()(layer1)\n",
    "    layer1 = Dropout(dropout_rate)(layer1)\n",
    "\n",
    "    # Layer 2 is a dense layer of length 10 for each facial feature\n",
    "    layer2 = Dense(10,activation=activation)(layer1)\n",
    "    # Flatten the layer to prepare for the output\n",
    "    layer2 = Flatten()(layer2)\n",
    "\n",
    "    # Layer three is the output layer\n",
    "    layer3 = Dense(2, activation=\"softmax\")(layer2)\n",
    "\n",
    "    # define the model\n",
    "    model = Model([left_eye.input, right_eye.input, left_eyebrow.input,right_eyebrow.input,nose.input,mouth.input,\n",
    "                   face_contour.input,iris.input, left_line.input, right_line.input], outputs=layer3)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_loss(model_training,emotion, user):\n",
    "    stats = pd.DataFrame(model_training.history)\n",
    "    stats.reset_index(level=0, inplace=True)\n",
    "    stats = stats.melt(id_vars =['index'])\n",
    "    stats.columns= ['Epoch','metric', 'Value']\n",
    "\n",
    "    fig = px.line(stats,x='Epoch', y=\"Value\", color='metric',title=f\"Multi-NN Model Statistics for {emotion}: {user}\")\n",
    "    fig.update_yaxes(range=[0, 1])\n",
    "\n",
    "    return fig   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-connecticut",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "model_neg_usera_multi = multi_NN_model()\n",
    "\n",
    "# fit model\n",
    "trained = model_neg_usera_multi.fit(train,y_neg_onehot,epochs=80,validation_split=.2, verbose=False)\n",
    "\n",
    "# plot performance\n",
    "plot_loss(trained, \"Negative\",\"User A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-tribe",
   "metadata": {},
   "source": [
    "Model performance of the multi-input NN after training on User A indicates after 20 epochs the model risks overfitting. Therefore use 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine final model\n",
    "model_neg_usera_multi = multi_NN_model()\n",
    "\n",
    "# fit final model\n",
    "final_model = model_neg_usera_multi.fit(train,y_neg_onehot,epochs=20,validation_split=.2, verbose=False)\n",
    "\n",
    "# save model weights\n",
    "#model_neg_usera_multi.save_weights(\"models/mnn_neg_usera.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model_neg_usera_multi.load_weights(\"models/mnn_neg_usera.h5\")\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_neg_usera_multi.predict(train_userb)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_neg_userb, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_neg_userb, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_neg_userb, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_neg_userb, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-forty",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.6289506953223767    \n",
    "Model Precision: 0.6075731497418244    \n",
    "Model Recall: 0.4957865168539326    \n",
    "Model AUC Score: 0.6168587756683455   \n",
    "\n",
    "Overall the multi-input NN model performed similairly to the original NN scoring 62.9% accuracy, 60% precision, and 50% recall.\n",
    "### Train/Test Improved Model on Emphasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data by x,y, and z coordinates\n",
    "xs_emp = X_emp_scaled[:,0::3]\n",
    "ys_emp = X_emp_scaled[:,1::3]\n",
    "zs_emp = X_emp_scaled[:,2::3]\n",
    "\n",
    "# combine data into 3-dimensional array shape (# of rows, # of dimensions, number of datapoints per dimension)\n",
    "X_emp_scaled_coord = np.swapaxes(np.array((xs_emp,ys_emp,zs_emp)),0,1)\n",
    "\n",
    "# Perform on User B Emphasis\n",
    "xs_emp_userb = X_emp_scaled_userb[:,0::3]\n",
    "ys_emp_userb = X_emp_scaled_userb[:,1::3]\n",
    "zs_emp_userb = X_emp_scaled_userb[:,2::3]\n",
    "\n",
    "X_emp_scaled_coord_userb = np.swapaxes(np.array((xs_emp_userb,ys_emp_userb,zs_emp_userb)),0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For User A Emphasis\n",
    "left_eye_emp =  X_emp_scaled_coord[:,:,0:8]\n",
    "right_eye_emp = X_emp_scaled_coord[:,:,8:16]\n",
    "left_eyebrow_emp =  X_emp_scaled_coord[:,:,16:26]\n",
    "right_eyebrow_emp =  X_emp_scaled_coord[:,:,26:36]\n",
    "nose_emp = X_emp_scaled_coord[:,:,36:48]\n",
    "mouth_emp = X_emp_scaled_coord[:,:,48:68]\n",
    "face_contour_emp = X_emp_scaled_coord[:,:,68:87]\n",
    "iris_emp = X_emp_scaled_coord[:,:,87:90]\n",
    "left_line_emp = X_emp_scaled_coord[:,:,90:95]\n",
    "right_line_emp = X_emp_scaled_coord[:,:,95:100]\n",
    "\n",
    "# For User B Emphasis\n",
    "left_eye_emp_userb =  X_emp_scaled_coord_userb[:,:,0:8]\n",
    "right_eye_emp_userb = X_emp_scaled_coord_userb[:,:,8:16]\n",
    "left_eyebrow_emp_userb =  X_emp_scaled_coord_userb[:,:,16:26]\n",
    "right_eyebrow_emp_userb =  X_emp_scaled_coord_userb[:,:,26:36]\n",
    "nose_emp_userb = X_emp_scaled_coord_userb[:,:,36:48]\n",
    "mouth_emp_userb = X_emp_scaled_coord_userb[:,:,48:68]\n",
    "face_contour_emp_userb = X_emp_scaled_coord_userb[:,:,68:87]\n",
    "iris_emp_userb = X_emp_scaled_coord_userb[:,:,87:90]\n",
    "left_line_emp_userb = X_emp_scaled_coord_userb[:,:,90:95]\n",
    "right_line_emp_userb = X_emp_scaled_coord_userb[:,:,95:100]\n",
    "\n",
    "# define 10 facial features for \"Emphasis\"\n",
    "# create training instance\n",
    "train_emp = [left_eye_emp,right_eye_emp,left_eyebrow_emp,right_eyebrow_emp,nose_emp,mouth_emp,face_contour_emp,iris_emp,left_line_emp,right_line_emp]\n",
    "train_emp_userb = [left_eye_emp_userb,right_eye_emp_userb,left_eyebrow_emp_userb,right_eyebrow_emp_userb,nose_emp_userb,mouth_emp_userb,face_contour_emp_userb,iris_emp_userb, left_line_emp_userb,right_line_emp_userb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model_emp_usera_multi = multi_NN_model()\n",
    "\n",
    "# fit model\n",
    "trained = model_emp_usera_multi.fit(train_emp,y_emp_onehot,epochs=200,validation_split=.2)\n",
    "\n",
    "# plot performance\n",
    "plot_loss(trained,\"Emphasis\",\"User A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-support",
   "metadata": {},
   "source": [
    "The above performance chart suggests that the \"Multiple Input Nueral Network\" trained very well on the new dataset. After about ~20 epochs the model loss and validation loss begin to flatline along with accuracy. \n",
    "To avoid overfitting, use epochs = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST \n",
    "# determine final model\n",
    "model_emp_usera_multi =  multi_NN_model()\n",
    "final_model = model_emp_usera_multi.fit(train_emp,y_emp_onehot,epochs=80,validation_split=.2, verbose=False,shuffle=False)\n",
    "\n",
    "# save model weights\n",
    "#model_emp_usera_multi.save_weights(\"models/mnn_emp_usera.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model_emp_usera_multi.load_weights(\"models/mnn_emp_usera.h5\")\n",
    "\n",
    "y_pred = model_emp_usera_multi.predict(train_emp_userb)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_emp_userb, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_emp_userb, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_emp_userb, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_emp_userb, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-tragedy",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.8556547619047619    \n",
    "Model Precision: 0.8703296703296703     \n",
    "Model Recall: 0.7457627118644068    \n",
    "Model AUC Score: 0.8365959930785748   \n",
    "\n",
    "Model accuracy is 85.5%, with 87.0% precision and recall of 74.6%. The AUC is 83.7%. Overall this is very similar to the SVM model that produced 82.4% accuracy with 75% precision and 83.3% recall. Only the AUC for the SVM was far superior with a value of 0.89.\n",
    "\n",
    "## Train on User B and Test on User A.\n",
    "\n",
    "###### Train on User B Negative and Test on User A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg_userb_multi = multi_NN_model()\n",
    "trained = model_neg_userb_multi.fit(train_userb,y_neg_onehot_userb,epochs=80,validation_split=.2)\n",
    "plot_loss(trained,\"Negative\",\"User B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-capability",
   "metadata": {},
   "source": [
    "Model validation accuracy decreases rapidly and validation loss increases after 4-7 epochs, indicating that the model overfits. Overall this model did not train well, and therefore use epochs = 5 to avoid overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine final model\n",
    "model_neg_userb_multi =  multi_NN_model()\n",
    "final_model = model_neg_userb_multi.fit(train_userb,y_neg_onehot_userb,epochs=5,validation_split=.2, verbose=False)\n",
    "\n",
    "# save model \n",
    "#model_neg_userb_multi.save_weights(\"models/mnn_neg_userb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_neg_userb_multi.load_weights(\"models/mnn_neg_userb.h5\")\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_neg_userb_multi.predict(train)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_neg, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_neg, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_neg, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_neg, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-immunology",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.5907473309608541   \n",
    "Model Precision: 0.5841584158415841   \n",
    "Model Recall: 0.44696969696969696   \n",
    "Model AUC Score: 0.5825452511694122   \n",
    "\n",
    "The results above indicate that when the model trained on User B and tested on User A, it performed poorly in comparison to the reverse. However, this was expected given that the same occured when modelling using SVM. Overall accuracy, precision, recall, and AUC resulted in values between 45% - 59%. Suggests a random model.\n",
    "\n",
    "#### Train User B Emphasis and Test on User A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emp_userb_multi = multi_NN_model()\n",
    "trained = model_emp_userb_multi.fit(train_emp_userb,y_emp_onehot_userb,epochs=100,validation_split=.2)\n",
    "plot_loss(trained,\"Emphasis\",\"User B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-offense",
   "metadata": {},
   "source": [
    "Use 20 epochs since val_loss begins to increase after and accuracy flatlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine final model\n",
    "model_emp_userb_multi =  multi_NN_model()\n",
    "model_emp_userb_multi.fit(train_emp_userb,y_emp_onehot_userb,epochs=20,validation_split=.2, verbose=False)\n",
    "\n",
    "# save model\n",
    "#model_emp_userb_multi.save_weights('models/mnn_emp_userb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "#model_emp_userb_multi.load_weights('models/mnn_emp_userb.h5')\n",
    "\n",
    "# make predictions on test data and round\n",
    "y_pred = model_emp_userb_multi.predict(train_emp)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# undo hot encoding in order to test accuracy\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "# calculate model accuracy\n",
    "acc = accuracy_score(y_emp, y_pred)\n",
    "\n",
    "# calculate model precision\n",
    "prec = precision_score(y_emp, y_pred)\n",
    "\n",
    "# calculate model recall\n",
    "recall = recall_score(y_emp, y_pred)\n",
    "\n",
    "print(\"Model Accuracy:\", acc)\n",
    "print(\"Model Precision:\", prec)\n",
    "print(\"Model Recall:\", recall)\n",
    "\n",
    "print(\"Model AUC Score:\" ,roc_auc_score(y_emp, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-clerk",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.7248752672843906   \n",
    "Model Precision: 0.4125   \n",
    "Model Recall: 0.4     \n",
    "Model AUC Score: 0.6123951537744641   \n",
    "\n",
    "Overall model performance is poor for both training on \"Emphasis\" User B and testing on \"Emphasis\" User A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
